{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from process.water import water_process1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parents[2] / 'original_data/train'\n",
    "rain = pd.read_csv(data_dir / 'rainfall' / 'data.csv')\n",
    "rain_st = pd.read_csv(data_dir / 'rainfall' / 'stations.csv')\n",
    "tide = pd.read_csv(data_dir / 'tidelevel' / 'data.csv')\n",
    "tide_st = pd.read_csv(data_dir / 'tidelevel' / 'stations.csv')\n",
    "water = pd.read_csv(data_dir / 'waterlevel' / 'data.csv')\n",
    "water_st = pd.read_csv(data_dir / 'waterlevel' / 'stations.csv')\n",
    "dam = pd.read_csv(data_dir / 'dam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "\n",
    "def rain_process1(rain, rain_st):\n",
    "    ###################\n",
    "    # data.csv側の処理 #\n",
    "    ###################\n",
    "\n",
    "    ##### 全体でdrop_duplicates\n",
    "    rain.drop_duplicates(inplace=True)\n",
    "    \n",
    "    ##### (date, statition, city)が重複している行 -> \n",
    "    #####     その行の中でfloatが最も含まれている行を採用\n",
    "    # (date, station, city)でグループを作った時、2行以上ある場合は重複なのでそのindexだけ取り出す\n",
    "    nunique_date_st_city = rain.groupby(['date', 'station', 'city']).nunique()\n",
    "    nunique_date_st_city['max_count'] = nunique_date_st_city.max(axis=1)\n",
    "\n",
    "    dup_date_st_city = nunique_date_st_city.query('max_count >= 2')[['max_count']]\n",
    "    dup_date_st_city_idx =  dup_date_st_city.index\n",
    "    # 重複した行それぞれに含まれる値でfloatである値をカウント\n",
    "    dup_date_st_city_df = rain.set_index(['date', 'station', 'city']).loc[dup_date_st_city_idx]\n",
    "    dup_date_st_city_df['num_count'] = dup_date_st_city_df.apply(lambda x: \\\n",
    "        24 - pd.to_numeric(x, errors='coerce').isnull().sum(),axis=1)\n",
    "    # floatが最も多い1行だけをとり出してconcat\n",
    "    concat_df = None\n",
    "    for _, df in dup_date_st_city_df.groupby(['date', 'station', 'city']):\n",
    "        df = df.sort_values('num_count', na_position='first', ascending=False).iloc[0:1, :]\n",
    "        if concat_df is None:\n",
    "            concat_df = df.copy()\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df, df], axis=0)\n",
    "    concat_df.drop('num_count', inplace=True, axis=1)\n",
    "\n",
    "    # 重複していない行たちとconcat\n",
    "    unique_date_st_city = nunique_date_st_city.query('max_count == 1')\n",
    "    unique_date_st_city =  unique_date_st_city.index\n",
    "    unique_date_st_city_df = rain.set_index(['date', 'station', 'city']).loc[unique_date_st_city]\n",
    "    rain = pd.concat([unique_date_st_city_df, concat_df])\n",
    "\n",
    "    del nunique_date_st_city, dup_date_st_city, dup_date_st_city_df, \\\n",
    "        unique_date_st_city, unique_date_st_city_df, concat_df, df\n",
    "    gc.collect()\n",
    "\n",
    "    rain.reset_index(inplace=True)\n",
    "    rain.sort_values(['date', 'station', 'city'], inplace=True)\n",
    "\n",
    "    ##### (おそらく)同じstationである行の値をマージ\n",
    "    # 観測日数が31日のstationは、そのstation名に(電)のついたものと同じstationと考えられるのでマージ\n",
    "    for st in rain['station'].value_counts()[rain['station'].value_counts() == 31].index:\n",
    "        st_ = st + '(電)'\n",
    "        # 変更前後の２つのstationに含まれるデータに日付の重なりがなければマージ\n",
    "        if len(rain.query('station in (@st, @st_)')) == rain.query('station in (@st, @st_)')['date'].nunique():\n",
    "            rain.loc[rain['station'] == st, 'station'] = st_\n",
    "\n",
    "    # station.csvには存在しないstationで、(国)をつけたものなら存在するものはおなじstationとしてマージ\n",
    "    for st in set(rain['station'].unique()) - set(rain_st['観測所名称'].unique()):\n",
    "        bool_ = rain_st['観測所名称'].str.contains(st)\n",
    "        if (bool_).any():\n",
    "            st_ = rain_st[bool_]['観測所名称'].iloc[-1]\n",
    "            if f'{st}(国)' == st_:\n",
    "                # 変更前後の２つのstationに含まれるデータに日付の重なりがなければマージ\n",
    "                if len(rain.query('station in (@st, @st_)')) == rain.query('station in (@st, @st_)')['date'].nunique():\n",
    "                    rain.loc[rain['station'] == st, 'station'] = st_\n",
    "    \n",
    "    ######################\n",
    "    # station.csv側の処理 #\n",
    "    ######################\n",
    "\n",
    "    ##### station名に(砂防)が含まれているものは入力時使用も0であり、ないものとマージできる\n",
    "    rain_st.loc[:, '観測所名称'] = rain_st['観測所名称'].str.replace(r'\\(砂防\\)', '')\n",
    "\n",
    "    ###################\n",
    "    # データベースを作成 #\n",
    "    ###################\n",
    "    \n",
    "    # idに(station, city)を対応させたテーブルを作る\n",
    "    keys = rain.groupby(['station', 'city']).count().index\n",
    "    rain_db = pd.DataFrame(index=keys).reset_index()\n",
    "    rain_db['id'] = range(len(rain_db))\n",
    "    rain_db = rain_db.reindex(columns=['id', 'station', 'city'])\n",
    "\n",
    "    # column名に変更を加える\n",
    "    rain_st = rain_st.rename(columns={'観測所名称': 'station', '市町': 'city'})\n",
    "\n",
    "    # station.csvのcityがnanのもののうち、data.csvから埋められるものは埋める\n",
    "    for data in rain_st.iterrows(): # stationを一列ずつ取り出す\n",
    "        city = data[1]['city'] # cityを取り出す\n",
    "        if isinstance(city, float) and math.isnan(city): # そのcityがnanの時のみ\n",
    "            st = data[1]['station']\n",
    "            city = rain.query('station==@st')['city'].unique()[0] # data.csvからそのstationを検索してなんのcityかをみる\n",
    "            rain_st.loc[(rain_st['station'] == st), 'city'] = city\n",
    "    \n",
    "    # data.csvの(station, city)をidに置き換える\n",
    "    rain = rain_db.merge(rain, on=['station', 'city'], how='left')\n",
    "    rain.drop(['station', 'city'], axis=1, inplace=True)\n",
    "\n",
    "    rain_st = rain_db.merge(rain_st, on=['station', 'city'], how='left')\n",
    "    rain_st['入力時使用'] = rain_st['入力時使用'].fillna(0.0)\n",
    "\n",
    "    return rain, rain_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/kn1s0g_s407d23sqsmvzcql80000gn/T/ipykernel_48385/2105682670.py:59: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  bool_ = rain_st['観測所名称'].str.contains(st)\n",
      "/var/folders/s8/kn1s0g_s407d23sqsmvzcql80000gn/T/ipykernel_48385/2105682670.py:72: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  rain_st.loc[:, '観測所名称'] = rain_st['観測所名称'].str.replace(r'\\(砂防\\)', '')\n"
     ]
    }
   ],
   "source": [
    "rain_p1, rain_st_p1 = rain_process1(rain, rain_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "rain_db01 = pd.read_csv(Path.cwd().parents[2] / 'data/database01/rain_data.csv')\n",
    "rain_st_db01 = pd.read_csv(Path.cwd().parents[2] / 'data/database01/rain_station.csv')\n",
    "print(rain_p1.equals(rain_db01))\n",
    "print(rain_st_p1.equals(rain_st_db01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def water_process1(water, water_st):\n",
    "\n",
    "    water['station'] = water['station'].str.replace(r'\\(電\\)', '')\n",
    "\n",
    "    for st in set(water['station'].unique()) - set(water_st['観測所名称'].unique()):\n",
    "        bool_ = water_st['観測所名称'].str.contains(st)\n",
    "        if (bool_).any():\n",
    "            st_ = water_st[bool_]['観測所名称'].iloc[-1]\n",
    "            if f'{st}(国)' == st_:\n",
    "                if len(water.query('station in (@st, @st_)')) == water.query('station in (@st, @st_)')['date'].nunique():\n",
    "                    water.loc[water['station'] == st, 'station'] = st_\n",
    "    \n",
    "    water.loc[water['station'] == '山手', 'station'] = '山手(国)'\n",
    "\n",
    "    water_st.loc[:, '河川名'] = water_st['河川名'].str.replace('\\\\n', '')\n",
    "    \n",
    "    keys = water.groupby(['station', 'river']).count().index\n",
    "    water_db = pd.DataFrame(index=keys).reset_index()\n",
    "    water_db['id'] = range(len(water_db))\n",
    "    water_db = water_db.reindex(columns=['id', 'station', 'river'])\n",
    "\n",
    "    water = water_db.merge(water, on=['station', 'river'], how='left')\n",
    "    water.drop(['station', 'river'], axis=1, inplace=True)\n",
    "    \n",
    "    water_st = water_st.rename(columns={'観測所名称': 'station', '河川名': 'river'})\n",
    "    water_st = water_db.merge(water_st, on=['station', 'river'], how='left')\n",
    "\n",
    "    water_st.loc[water_st['river']=='太田川放水路', 'river'] = '太田川\\\\n放水路' # 改行があるとうまく処理されなかったので最後に改行を付け直す\n",
    "\n",
    "    return water, water_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/kn1s0g_s407d23sqsmvzcql80000gn/T/ipykernel_48385/4108981710.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  water['station'] = water['station'].str.replace(r'\\(電\\)', '')\n",
      "/var/folders/s8/kn1s0g_s407d23sqsmvzcql80000gn/T/ipykernel_48385/4108981710.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  bool_ = water_st['観測所名称'].str.contains(st)\n",
      "/var/folders/s8/kn1s0g_s407d23sqsmvzcql80000gn/T/ipykernel_48385/4108981710.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  water_st.loc[:, '河川名'] = water_st['河川名'].str.replace('\\\\n', '')\n"
     ]
    }
   ],
   "source": [
    "water_p1, water_st_p1 = water_process1(water, water_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tide_process1(tide, tide_st):\n",
    "    tide.loc[tide['station'] == '柿浦漁港', 'station'] = '柿浦港'\n",
    "    tide.loc[tide['station'] == '呉阿賀港', 'station'] = '呉(阿賀)港'\n",
    "    tide.loc[tide['station'] == '倉橋漁港', 'station'] = '倉橋港'\n",
    "\n",
    "    keys = tide.groupby(['station', 'city']).count().index\n",
    "    tide_db = pd.DataFrame(index=keys).reset_index()\n",
    "    tide_db['id'] = range(len(tide_db))\n",
    "    tide_db = tide_db.reindex(columns=['id', 'station', 'city'])\n",
    "\n",
    "    tide = tide_db.merge(tide, on=['station', 'city'], how='left')\n",
    "    tide.drop(['station', 'city'], axis=1, inplace=True)\n",
    "\n",
    "    tide_st = tide_st.rename(columns={'観測所名': 'station'})\n",
    "    tide_st = tide_db.merge(tide_st, on=['station'], how='left')\n",
    "    \n",
    "    return tide, tide_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_p1, tide_st_p1 = tide_process1(tide, tide_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timeseries(df):\n",
    "    all_id = df['id'].unique()\n",
    "    all_date = df['date'].unique().astype(int)\n",
    "    n_ids = len(all_id)\n",
    "    all_id.sort()\n",
    "    all_date.sort()\n",
    "    null_value = 'x'\n",
    "\n",
    "    data = []\n",
    "    for st in all_id:\n",
    "        for date in all_date:\n",
    "            data.append([date, st])\n",
    "    \n",
    "    old_df = df.copy()\n",
    "    old_df = old_df[~old_df[['date', 'id']].duplicated()]\n",
    "    old_df.fillna(null_value, inplace=True)\n",
    "\n",
    "    new_df = pd.DataFrame(data, columns=['date', 'id'])\n",
    "    new_df = pd.merge(new_df, old_df, on=['date', 'id'], how='left')\n",
    "    new_df = new_df.sort_values(['id', 'date']).reset_index(drop=True)\n",
    "    del old_df\n",
    "    \n",
    "    series = new_df.iloc[:, 2:].values.reshape(n_ids, -1).T\n",
    "    timeseries_df = pd.DataFrame(series, columns=all_id)\n",
    "\n",
    "    dates = []\n",
    "    hours = []\n",
    "    for date in all_date:\n",
    "        for hour in range(24):\n",
    "            dates.append(date)\n",
    "            hours.append(hour)\n",
    "    timeseries_df['date'] = dates\n",
    "    timeseries_df['hour'] = hours\n",
    "    timeseries_df = timeseries_df.reindex(columns=['date', 'hour'] + list(all_id))\n",
    "    return timeseries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_t = convert_timeseries(rain_p1)\n",
    "water_t = convert_timeseries(water_p1)\n",
    "tide_t = convert_timeseries(tide_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
